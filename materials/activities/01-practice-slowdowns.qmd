---
title: "Find the slowdowns"
editor: source
---

## Setup

```{r}
library(tidyverse)
library(glue)
library(tictoc)
```


## Your Turn

We will find the slowdowns in the following process:

```{r}

states <- c("ak", "al", "ar", "az", "wa", "wi", "wv", "wy")

dat <- data.frame()

for (state in states) {
  my_files <- list.files(glue("../data/raw_csvs/person/2021/{state}/"), full.names = TRUE)
  
  for (file in my_files) {
    
    temp <- read_csv(file) |>
      mutate_all(as.character)
    
    dat <- dat |>
      bind_rows(temp)
    
  }
}

```

```{r}

dat |>
  mutate(
    age_groups = case_when(
      AGEP < 18 ~ "Under 18",
      AGEP < 65 ~ "18-64",
      TRUE ~ "65+"
    )
  ) |>
  group_by(REGION, CIT, SEX, ST, age_groups) |>
  summarize(
    count = n()
  ) |>
  pivot_wider(names_from = age_groups,
              values_from = count) |>
  filter(REGION == 3) 


```

### Step One: Find the problem chunk(s)

Use `tictoc()` to figure out which of the two steps is slowest.

Use `system.time()` to figure out which part of the data read is slowest.

### Step Two: Profile the processes

Use `profviz()` to see where the slowdowns occur in the analysis pipeline.

### Step Three: Benchmark a solution

Try rearranging your analysis pipeline to be more efficient.  Compare this solution to the original.



