---
title: "Finding slowdowns and solutions."
format: 
  revealjs:
    footer: "[JSM: Large Data](https://github.com/kbodwin/jsm-big-data-2025/tree/main/materials)"
    theme: simple
    scrollable: true
    embed-resources: true
editor: source
execute:
  echo: true
---

# "Local Data" {background-color="#0F4C81"}

## You have data that...

* You want to analyze: summarize, visualize, model, etc.

* Can be downloaded somewhere on your local machine.

* Can be read fully into R...

. . .

 ... but maybe very slowly...
 
. . . 

 ... and maybe only if it's a *parquet* file.



## What if it's bigger than that?

* It's probably in a **cloud database**. (e.g. AWS)

* Do the *individual* files/tables fit on disk?

* Can you query *subsets* of the data and fit those on disk?


# Vocab {background-color="#0F4C81"}

## Key Terms

* Data is **on disk** if it is stored on your computer

* Data is **in-memory** if you load it into RAM, e.g. loading into R.

* A **csv** file is a file type for storing data as *comma separated text*.

* A **parquet** file is a file type for storing data as *column information*.



## Key Packages

* `data.table` optimizes calculations **in R** on data frames, via algorithmic cleverness and **C implementation**.

* `duckdb` creates a **SQL database** locally and lets you use **R Code** to execute **SQL operations**.

* `arrow` provides ways to read and write **parquet files** and to move data around between `data.table`, `duckdb`, and other formats.

## Helper Packages

* Friends of `data.table`:  
    + `dtplyr`, `tidyfast` for `dplyr` syntax
    + `mlr3` for machine learning

* Friends of `duckdb`:
    + `duckplyr` for for `dplyr` syntax
    + `odbc`,  for connection to cloud databases

* Other speed/efficiency helpers: 
    + `polars` in python




# Possible Slowdowns {background-color="#0F4C81"}

## 1. My data reads in slowly.

A. Use `data.table::fread()` instead.

B. Write it to a *parquet* version; use `arrow::read_parquet()`.

C. Put it in a *duckdb*; use queries to avoid reading the whole dataset at once.


## 2. One of my pipelines is a little slow, and I do it many times

A. Are you using *vectorized* functions (or could you)?

B. Use `data.table` - do the small speed gains add up?

C. Can you move some subsetting steps to `duckdb`?


## 3. One of my pipelines is very slow.

A. Can you re-order the pipeline?

B. Are you doing a *split-apply-combine* over many groups?  `data.table`!

C. Are you doing a *subsetting* process? `data.table` or `duckdb`!

D. Is it just a lot of data?  `duckdb` and calculate in partitions.

## Setup

```{r}
#| include: false
library(tidyverse)
library(microbenchmark)
library(bench)
library(tictoc)
library(atime)
library(profvis)
```

```{r}
dat <- read_csv("../data/raw_csvs/person/2021/az/psam_p04.csv")
```



# Step 1. Timing Chunks {background-color="#0F4C81"}


## The `tictoc()` package

```{r}
#| cache: true
tic()

  dat |>
    pivot_longer(PWGTP1:PWGTP80,
               names_to = "Weight_Num",
               values_to = "Weight_Amount") |>
    group_by(ST) |>
    mutate(
      max_weight = max(Weight_Amount)
    )

toc()
  
```





# Step 2. Profiling Processes {background-color="#0F4C81"}

```{r}
#| cache: true
profvis::profvis({
  dat |>
    pivot_longer(PWGTP1:PWGTP80,
               names_to = "Weight_Num",
               values_to = "Weight_Amount") |>
    group_by(ST) |>
    mutate(
      max_weight = max(Weight_Amount)
    )
}
)
```


# Step 3. Benchmarking Solutions {background-color="#0F4C81"}

## Function wrappers (optional)

```{r}
#| cache: true

old_pipeline <-  function() {
  dat |>
    pivot_longer(PWGTP1:PWGTP80,
               names_to = "Weight_Num",
               values_to = "Weight_Amount") |>
    group_by(CIT) |>
    mutate(
      max_weight = max(Weight_Amount)
    )
}

library(dtplyr)

new_pipeline <- function() {
  dat |>
    rowwise() |>
    summarize(
      max_weight = max(PWGTP1:PWGTP80),
      CIT = CIT
    ) |>
    group_by(CIT) |>
    summarize(
      max_weight = max(max_weight)
    )
}
```

## Microbenchmark for multiple runs

```{r}
#| cache: true
microbenchmark::microbenchmark(
  old_version = old_pipeline(),
  new_version = new_pipeline(),
  times = 5
)
```

## `bench` package for memory comparisons...


```{r}
#| cache: true
bench::mark(
  old_version = old_pipeline(),
  new_version = new_pipeline(),
  check = FALSE,
  max_iterations = 3
)
```

## ... and testing scaling with size

```{r}
#| cache: true
results <- bench::press(
  duplications = c(1, 2),
  {
    dat_big <- bind_rows(replicate(duplications, dat, simplify = FALSE))
    bench::mark(
      old_version = old_pipeline(),
      new_version = new_pipeline(),
      check = FALSE,
      max_iterations = 3
    )
  }
)

results
```


## Summary

1. Use `tictoc()` or `proc.time()` to get a feel for runtimes of larger chunks.

2. Use profiling to narrow down where in a chunk the slowdowns are.

3. Use benchmarking to compare your old version to a proposed solution.

4. Use benchmark testing to see how your speed ups scale with data size.
